{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: (1000000, 59)\n",
      "Testing Data: (303605, 59)\n",
      "{' 60 months': 0, 'E': 1, 'RENT': 2, 'Verified': 3, 'D': 4, 'MORTGAGE': 5, 'Source Verified': 6, ' 36 months': 7, 'B': 8, 'Not Verified': 9, 'A': 10, 'F': 11, 'C': 12, 'OWN': 13, 'G': 19, 'ANY': 15, 'OTHER': 16, 'NONE': 17, 'AS': 20, 'DC': 21, 'FM': 22, 'GU': 23, 'MH': 24, 'MP': 25, 'PW': 26, 'PR': 27, 'VI': 28, 'AL': 29, 'AK': 30, 'AZ': 31, 'AR': 32, 'CA': 33, 'CO': 34, 'CT': 35, 'DE': 36, 'FL': 37, 'GA': 38, 'HI': 39, 'ID': 40, 'IL': 41, 'IN': 42, 'IA': 43, 'KS': 44, 'KY': 45, 'LA': 46, 'ME': 47, 'MD': 48, 'MA': 49, 'MI': 50, 'MN': 51, 'MS': 52, 'MO': 53, 'MT': 54, 'NE': 55, 'NV': 56, 'NH': 57, 'NJ': 58, 'NM': 59, 'NY': 60, 'NC': 61, 'ND': 62, 'OH': 63, 'OK': 64, 'OR': 65, 'PA': 66, 'RI': 67, 'SC': 68, 'SD': 69, 'TN': 70, 'TX': 71, 'UT': 72, 'VT': 73, 'VA': 74, 'WA': 75, 'WV': 76, 'WI': 77, 'WY': 78}\n",
      "[1 1 1 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "##### EGE BİRCAN  20950\n",
    "##### ATAKAN ATİK 20537\n",
    "\n",
    "##### CS412 MACHINE LEARNING PROJECT\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Reading csv files from according paths\n",
    "testData = pd.read_csv(\"/home/egebircan/Desktop/loansTest.csv\", header=None, skiprows=1)\n",
    "trainData = pd.read_csv(\"/home/egebircan/Desktop/loansTrain.csv\", header=None, skiprows=1)\n",
    "\n",
    "# Analyzing the shapes to control if they are fittable\n",
    "print('Training Data:', trainData.shape)\n",
    "print('Testing Data:', testData.shape)\n",
    "\n",
    "\n",
    "# In training data there were 1 million rows and in test data there were 300k rows. And in training data there\n",
    "# were at most 130k nan values for a single column. Which is not too much with respect to 1 millon lines. Also the\n",
    "# same situation occured with test data. So instead of filling the nan values with mean or trying to edit them and\n",
    "# sacrifice our processing power and time, we chose filling them with 0's to gain time and processing power. Since\n",
    "# nan values were not too many with respect to the data sizes, we thought it is an efficient decision.\n",
    "trainData.fillna(0, inplace=True)\n",
    "testData.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# Holding a dictionary to fill with the unique values inside data columns\n",
    "dictionary = dict()\n",
    "counter = 0\n",
    "\n",
    "\n",
    "# Since our algorithm Decision Tree cannot deal with strings, we had encode them somehow. We decided to use the built\n",
    "# in encoder for python pandas but then we realised that pandas encoder encodes train and test data different. Since each\n",
    "# of them has different unique values and some values that occur in train data does not occur in test data(or vice-versa)\n",
    "# So to overcome this problem we decided to hold a dictionary obtained from training data, and apply the key's values to\n",
    "# test data. This way both of the datas would be encoded using the same key value pairs.\n",
    "\n",
    "# Obtaining the dictionary from training data:\n",
    "for x in range (0, 1000000):\n",
    "    if not trainData.iloc[x,1] in dictionary.keys(): \n",
    "        dictionary[trainData.iloc[x,1]] = counter\n",
    "        counter = counter + 1 \n",
    "    if not trainData.iloc[x,2] in dictionary.keys():\n",
    "        dictionary[trainData.iloc[x,2]] = counter\n",
    "        counter = counter + 1     \n",
    "    if not trainData.iloc[x,5] in dictionary.keys():\n",
    "        dictionary[trainData.iloc[x,5]] = counter\n",
    "        counter = counter + 1 \n",
    "    if not trainData.iloc[x,7] in dictionary.keys():\n",
    "        dictionary[trainData.iloc[x,7]] = counter\n",
    "        counter = counter + 1 \n",
    "\n",
    "        \n",
    "# Here we realised that training data does not have credit grade G but the test data has it. So we decided to add grade \n",
    "# G hardcoded to save from time and efficiency. After adding it to the dictionary we gave it a value using counter variable.\n",
    "dictionary[\"G\"] = counter + 1\n",
    "counter = counter + 1\n",
    "\n",
    "\n",
    "# Traversing the training data and obtaining state codes were not efficient since there are only a small \n",
    "# limited number of them. We decided to add them to the dictionary and gave values hardcoded.\n",
    "stateCodes = [\"AS\", \"DC\", \"FM\", \"GU\", \"MH\", \"MP\", \"PW\", \"PR\", \"VI\", \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\n",
    "for x in stateCodes:\n",
    "    dictionary[x] = counter + 1\n",
    "    counter = counter + 1\n",
    "print(dictionary)\n",
    "trainAttributes = list()\n",
    "trainResult = list()\n",
    "testAttributes = list()\n",
    "\n",
    "\n",
    "# Since we finished encoding our data using the dictionaries, it is time to form data arrays obtained from encoded data.\n",
    "# in this algorithm we get the value from the data, check our dictionary if the data obtained is a key in the dictionary.\n",
    "# if it exists in the dictionary that means the value is string and it is encoded. We use the key's value in the actual\n",
    "# data array so that our actual data frame will be encoded too. If the value obtained from the data does not exist in the\n",
    "# dictionary it means that the value is integer/float. So we just add it to data frame just as it is.\n",
    "\n",
    "for x in range (0, 1000000):\n",
    "    attributeRowList = list()\n",
    "    # in the if lines below, we got rid of the low correlation columns to make our machine learning algorithm\n",
    "    # more accurate and save from running time and gain efficiency.\n",
    "    for y in range (0, 58):\n",
    "        if y != 3 and y != 4 and y != 8 and y != 9 and y != 13 and y != 20:\n",
    "            if trainData.iloc[x,y] in dictionary.keys(): \n",
    "                #print(\"dictionary'de kayıtlı\")\n",
    "                attributeRowList.append(dictionary[trainData.iloc[x,y]])\n",
    "            else:\n",
    "                attributeRowList.append(trainData.iloc[x,y])\n",
    "        \n",
    "    trainAttributes.append(attributeRowList)\n",
    "    trainResult.append(trainData.iloc[x,58])\n",
    "\n",
    "#print(trainResult)\n",
    "\n",
    "for x in range (0, 303605):\n",
    "    testAttributeRowList = list()\n",
    "    for y in range (1, 59):\n",
    "        if y != 4 and y != 5 and y != 9 and y != 10 and y != 14 and y != 21:\n",
    "            if testData.iloc[x,y] in dictionary.keys(): \n",
    "                #print(\"dictionary'de kayıtlı\")\n",
    "                testAttributeRowList.append(dictionary[testData.iloc[x,y]])\n",
    "            else:\n",
    "                testAttributeRowList.append(testData.iloc[x,y])\n",
    "\n",
    "    testAttributes.append(testAttributeRowList)    \n",
    "    \n",
    "\n",
    "\n",
    "X_train = trainAttributes\n",
    "y_train = trainResult\n",
    "X_test = testAttributes\n",
    "\n",
    "# We chose decision tree algorithm because the data we are working on had some string values. And uniqueness of these\n",
    "# values were very low. For example home_ownership column consists of string values but the values are only 3 types.\n",
    "# rent, mortgage and own. verification_status column had also the same thing. It only consists of 3 types that are \n",
    "# verified, not verified, source verified. Since these are the qualities of the data, we thought it would be easy\n",
    "# and efficient to form different classes out of these string variables after encoding.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_predict = dt.predict(X_test)\n",
    "\n",
    "print(y_predict)   \n",
    "\n",
    "idList = list()\n",
    "for x in range (0,303605):\n",
    "    idList.append(x)\n",
    "\n",
    "rows = zip(idList, y_predict)\n",
    "\n",
    "import csv\n",
    "\n",
    "# Outputting the rows to a csv file as the sample output.\n",
    "\n",
    "with open(\"/home/egebircan/Desktop/loanPredictions.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in rows:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
